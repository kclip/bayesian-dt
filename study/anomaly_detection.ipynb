{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4c629c",
   "metadata": {},
   "source": [
    "# Paper study - Anomaly detection\n",
    "\n",
    "## Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a712473",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "\n",
    "from settings import Metadata, EnvMetadata, ExperimentPhase, \\\n",
    "    PROJECT_FOLDER, DATA_FOLDER, TEST_EPISODES_FOLDER, setup_matplotlib_config\n",
    "from bin.main import run_simulation\n",
    "from src.utils import lineplot_ci\n",
    "from src.data_classes import Episode\n",
    "from src.environment.channel.utils import create_K_MPR_matrix\n",
    "from src.view.plot import plot_episode, plot_validation_metrics, plot_per_step_metrics, plot_validation_metrics_per_n_steps_model_learning, \\\n",
    "    plot_p_transmit_aac, plot_critic_value_aac, plot_training_actor_critic, plot_coma_actors_critic, \\\n",
    "    plot_tdma_actors_intermediary_probabilities\n",
    "from src.view.plot_model import plot_dirichlet_data_generation, plot_dirichlet_mpr_channel\n",
    "from src.view.metrics import get_experiment_throughput, get_experiment_fairness, experiment_throughput_mean_dev, get_return, \\\n",
    "    get_buffer_info, get_channel_collisions, get_aac_info, get_training_info, get_state_distribution,  \\\n",
    "    get_coma_info, get_loss, get_critic_training_info, get_gradients_info, estimate_Q_value_COMA\n",
    "\n",
    "setup_matplotlib_config()\n",
    "\n",
    "root = logging.getLogger()\n",
    "if root.handlers:\n",
    "    for handler in root.handlers:\n",
    "        root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s %(message)s',level=logging.INFO)\n",
    "\n",
    "def does_experiment_exist(experiment_name):\n",
    "    return os.path.isdir(os.path.join(PROJECT_FOLDER, DATA_FOLDER, experiment_name))\n",
    "\n",
    "def does_experiment_test_exist(experiment_name):\n",
    "    return os.path.isdir(os.path.join(PROJECT_FOLDER, DATA_FOLDER, experiment_name, TEST_EPISODES_FOLDER))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff82f11",
   "metadata": {},
   "source": [
    "## Experimental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from experimental_setup import *\n",
    "\n",
    "# Log-likelihood\n",
    "EPSILON = 10**(-10)  # To avoid log(0)\n",
    "\n",
    "# Number of steps used to create the in-distribution and out-of-distribution datasets\n",
    "N_STEPS_DATA = 8000\n",
    "\n",
    "\n",
    "# Perturbed system : we collect out-of-distribution data by simulating a perturbed environment where only\n",
    "# the data generation probabilities differ from the initial system.\n",
    "# Perturbed data generation probabilities\n",
    "PERTURBED_DATA_GEN_PROBABILITIES_MAP = None  # No time-dependent data generation\n",
    "PERTURBED_DEFAULT_JOINT_DISTRIBUTION = {\n",
    "    \"0,0\": 0.6,\n",
    "    \"1,0\": 0.4,\n",
    "    \"0,1\": 0,  # Device is disconnected\n",
    "    \"1,1\": 0\n",
    "}\n",
    "PERTURBED_DATA_GEN_PROBABILITIES_MAPS_KWARGS = [\n",
    "    {\n",
    "        \"name\": \"cluster_1\",\n",
    "        \"n_joint_agents\": 2,\n",
    "        \"probabilities_map\": PERTURBED_DATA_GEN_PROBABILITIES_MAP,\n",
    "        \"default_joint_distribution\": PERTURBED_DEFAULT_JOINT_DISTRIBUTION\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cluster_2\",\n",
    "        \"n_joint_agents\": 2,\n",
    "        \"probabilities_map\": PERTURBED_DATA_GEN_PROBABILITIES_MAP,\n",
    "        \"default_joint_distribution\": PERTURBED_DEFAULT_JOINT_DISTRIBUTION\n",
    "    }\n",
    "]\n",
    "\n",
    "# Number of samples used for tests\n",
    "N_TEST_SAMPLES = 100  # n samples per data distribution (in-dist and out-dist)\n",
    "N_STEPS_PER_SAMPLE = 5\n",
    "\n",
    "\n",
    "# Log\n",
    "EXPERIMENT_NAME_PREFIX = \"paper_policy_monitor\"\n",
    "\n",
    "\n",
    "# Print\n",
    "_JOINT_SINGLE_PACKET_GENERATED_PROBABILITY = DEFAULT_JOINT_DISTRIBUTION[\"1,0\"]\n",
    "_MARGINAL_DATA_GEN_PROBABILITY = _JOINT_SINGLE_PACKET_GENERATED_PROBABILITY / (1 + _JOINT_SINGLE_PACKET_GENERATED_PROBABILITY)\n",
    "print(f\"Marginal packet generation probability per agent : {_MARGINAL_DATA_GEN_PROBABILITY}\")\n",
    "print(f\"Joint node distribution : {DEFAULT_JOINT_DISTRIBUTION}\")\n",
    "print(f\"MPR Matrix : {MPR_MATRIX}\")\n",
    "\n",
    "\n",
    "\n",
    "# Previous experiments\n",
    "# EXPERIMENT_VERSION = \"02\"\n",
    "# PERTURBED_DEFAULT_JOINT_DISTRIBUTION = {\n",
    "#     \"0,0\": 0.6,\n",
    "#     \"1,0\": 0.2,\n",
    "#     \"0,1\": 0.2,\n",
    "#     \"1,1\": 0\n",
    "# }\n",
    "\n",
    "# EXPERIMENT_VERSION = \"03\"\n",
    "# PERTURBED_DEFAULT_JOINT_DISTRIBUTION = {\n",
    "#     \"0,0\": 0.2,\n",
    "#     \"1,0\": 0.8,\n",
    "#     \"0,1\": 0,  # Device is disconnected\n",
    "#     \"1,1\": 0\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4efd8",
   "metadata": {},
   "source": [
    "## Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_VERSION = \"04\"  # Version of runs\n",
    "\n",
    "N_MODELS_PER_STEP = 50\n",
    "N_STEPS_LIST = [10, 20, 50]\n",
    "RANGE_MODEL_STEPS = list(product(range(N_MODELS_PER_STEP), N_STEPS_LIST))\n",
    "\n",
    "def get_metadata_model_learning(n_steps):\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 0,\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"posterior_sample\",\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 1,\n",
    "            \"train_model_max_steps\": n_steps,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "def get_model_learning_experiment_name(n_model, n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_model_learning_random_{EXPERIMENT_VERSION}_n_model_{n_model}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d909e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    metadata_model_learning = get_metadata_model_learning(n_steps)\n",
    "    experiment_name_model_learning = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    if does_experiment_exist(experiment_name_model_learning):\n",
    "        print(f\"Experiment '{experiment_name_model_learning}' already done...\")\n",
    "    else:\n",
    "        run_simulation(\n",
    "            metadata_model_learning,\n",
    "            log=True,\n",
    "            log_train=True,\n",
    "            log_experiment_name=experiment_name_model_learning,\n",
    "            suffix_log_experiment_name=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec80d68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episode_max_steps = Episode.load_experiment(\n",
    "    get_model_learning_experiment_name(2, 50),\n",
    "    experiment_phase=ExperimentPhase.TRAIN_MODEL\n",
    ")[0]\n",
    "plot_dirichlet_data_generation(\n",
    "    episode_max_steps,\n",
    "    \"cluster_1\",\n",
    "    prior_dirichlet_concentration=PRIOR_DIRICHLET_CONCETRATION,\n",
    "    prior_dirichlet_concentration_map=PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "    n_steps_per_plot=1,\n",
    "    true_transition_probability={\"\": DEFAULT_JOINT_DISTRIBUTION},\n",
    "    x_axis_range=[0, 1],\n",
    "    x_axis_step=0.001\n",
    ")\n",
    "plot_dirichlet_data_generation(\n",
    "    episode_max_steps,\n",
    "    \"cluster_2\",\n",
    "    prior_dirichlet_concentration=PRIOR_DIRICHLET_CONCETRATION,\n",
    "    prior_dirichlet_concentration_map=PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "    n_steps_per_plot=1,\n",
    "    true_transition_probability={\"\": DEFAULT_JOINT_DISTRIBUTION},\n",
    "    x_axis_range=[0, 1],\n",
    "    x_axis_step=0.001\n",
    ")\n",
    "plot_dirichlet_mpr_channel(\n",
    "    episode_max_steps,\n",
    "    prior_dirichlet_concentration=PRIOR_DIRICHLET_CONCETRATION,\n",
    "    prior_dirichlet_concentration_map=PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "    n_steps_per_plot=1,\n",
    "    max_packets_transmitted=4,\n",
    "    true_mpr_matrix=MPR_MATRIX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e896af2",
   "metadata": {},
   "source": [
    "## Collect data\n",
    "\n",
    "### Train operational policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_policy_opt_on_model(lr_actor=POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"]):\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 0\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"posterior_sample\",\n",
    "                \"n_steps_between_model_update\": N_STEPS_BETWEEN_POSTERIOR_SAMPLE,\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": N_EPISODES_TRAINING_POLICY,\n",
    "            \"train_policy_max_steps\": N_STEPS_TRAINING_POLICY,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": {\n",
    "                **POLICY_OPTIMIZER_METADATA,\n",
    "                \"learning_rate_actor\": lr_actor\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_policy_opt_on_model(n_model, n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_policy_opt_on_model_{EXPERIMENT_VERSION}_n_model_{n_model}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d8bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    policy_opt_experiment_name = get_experiment_name_policy_opt_on_model(n_model, n_steps)\n",
    "    model_experiment_name = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    metadata = get_metadata_policy_opt_on_model()\n",
    "    if does_experiment_exist(policy_opt_experiment_name):\n",
    "        print(f\"Experiment '{policy_opt_experiment_name}' already done...\")\n",
    "    else:\n",
    "        try:\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=False,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                suffix_log_experiment_name=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"ERROR ! Trying with half the learning rate for the actor...\")\n",
    "            if does_experiment_exist(policy_opt_experiment_name):\n",
    "                os.rename(\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, policy_opt_experiment_name),\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{policy_opt_experiment_name}_ERROR\"),\n",
    "                )\n",
    "            new_actor_lr = POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"] / 2\n",
    "            metadata = get_metadata_policy_opt_on_model(new_actor_lr)\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=False,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                suffix_log_experiment_name=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57922519",
   "metadata": {},
   "source": [
    "### Select policy with highest reward for each model learning dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate est episode\n",
    "TEST_EPISODE_NAME = f\"{EXPERIMENT_NAME_PREFIX}_test_episode_{EXPERIMENT_VERSION}\"\n",
    "\n",
    "def get_metadata_generate_test_episode():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 1\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinPolicyPassthrough\",\n",
    "            \"digital_twin_kwargs\": {},\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerAloha\",\n",
    "            \"policy_optimizer_kwargs\": {\"p_transmit\": 1 / N_AGENTS}\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Test policy\n",
    "def get_metadata_test_policy():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 0\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinPolicyPassthrough\",\n",
    "            \"digital_twin_kwargs\": {},\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1decf6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if does_experiment_exist(TEST_EPISODE_NAME):\n",
    "    print(f\"Experiment '{TEST_EPISODE_NAME}' already done...\")\n",
    "else:\n",
    "    run_simulation(\n",
    "        get_metadata_generate_test_episode(),\n",
    "        log=True,\n",
    "        log_train=False,\n",
    "        log_experiment_name=TEST_EPISODE_NAME,\n",
    "        suffix_log_experiment_name=False\n",
    "    )\n",
    "\n",
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    policy_experiment_name = get_experiment_name_policy_opt_on_model(n_model, n_steps)\n",
    "    if does_experiment_test_exist(policy_experiment_name):\n",
    "        print(f\"Experiment '{policy_experiment_name}' policy already tested...\")\n",
    "    else:\n",
    "        run_simulation(\n",
    "            get_metadata_test_policy(),\n",
    "            log=True,\n",
    "            log_train=False,\n",
    "            log_experiment_name=policy_experiment_name,\n",
    "            load_policy_experiment_name=policy_experiment_name,\n",
    "            load_forced_test_experiment_name=TEST_EPISODE_NAME,\n",
    "            suffix_log_experiment_name=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f07ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_POLICY = dict()\n",
    "\n",
    "light_selection = {\n",
    "    \"rewards\": True,\n",
    "    \"actions\": False,\n",
    "    \"info\": {},\n",
    "    \"state\": False,\n",
    "    \"digital_twin_info\": {},\n",
    "    \"train_info\": {}\n",
    "}\n",
    "current_best = dict()\n",
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    policy_experiment_name = get_experiment_name_policy_opt_on_model(n_model, n_steps)\n",
    "    episode = Episode.load_episode(\n",
    "        policy_experiment_name,\n",
    "        \"ep_0\",\n",
    "        experiment_phase=ExperimentPhase.TEST_POLICY\n",
    "    )\n",
    "    total_reward = np.sum([step.rewards for step in episode.history])\n",
    "    if (\n",
    "        (current_best.get(n_steps, None) is None) or\n",
    "        (current_best[n_steps] < total_reward)\n",
    "    ):\n",
    "        current_best[n_steps] = total_reward\n",
    "        SELECTED_POLICY[n_steps] = policy_experiment_name\n",
    "\n",
    "print(SELECTED_POLICY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61376f",
   "metadata": {},
   "source": [
    "### Collect in-distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_in_distribution_data_collection():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 1,\n",
    "            \"test_max_steps\": N_STEPS_DATA\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"posterior_sample\",\n",
    "                \"n_steps_between_model_update\": N_STEPS_BETWEEN_POSTERIOR_SAMPLE,\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_in_distribution_data_collection(n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_in_distribution_data_collection_{EXPERIMENT_VERSION}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375fa12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_steps in N_STEPS_LIST:\n",
    "    data_collect_experiment_name = get_experiment_name_in_distribution_data_collection(n_steps)\n",
    "    policy_experiment_name = SELECTED_POLICY[n_steps]\n",
    "    metadata = get_metadata_in_distribution_data_collection()\n",
    "    if does_experiment_exist(data_collect_experiment_name):\n",
    "        print(f\"Experiment '{data_collect_experiment_name}' already done...\")\n",
    "    else:\n",
    "        run_simulation(\n",
    "            metadata,\n",
    "            log=True,\n",
    "            log_train=False,\n",
    "            log_experiment_name=data_collect_experiment_name,\n",
    "            load_policy_experiment_name=policy_experiment_name,\n",
    "            suffix_log_experiment_name=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a3a6d",
   "metadata": {},
   "source": [
    "### Collect out-of-distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_out_of_distribution_data_collection():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 1,\n",
    "            \"test_max_steps\": N_STEPS_DATA,\n",
    "            \"data_generator_probabilities_maps_kwargs\": PERTURBED_DATA_GEN_PROBABILITIES_MAPS_KWARGS,\n",
    "            \"data_generator_dependencies_kwargs\": DATA_GEN_DEPENDENCIES_KWARGS,  # We keep the same cluster topology\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"posterior_sample\",\n",
    "                \"n_steps_between_model_update\": N_STEPS_BETWEEN_POSTERIOR_SAMPLE,\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_out_of_distribution_data_collection(n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_out_of_distribution_data_collection_{EXPERIMENT_VERSION}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c131a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_steps in N_STEPS_LIST:\n",
    "    data_collect_experiment_name = get_experiment_name_out_of_distribution_data_collection(n_steps)\n",
    "    policy_experiment_name = SELECTED_POLICY[n_steps]\n",
    "    metadata = get_metadata_out_of_distribution_data_collection()\n",
    "    if does_experiment_exist(data_collect_experiment_name):\n",
    "        print(f\"Experiment '{data_collect_experiment_name}' already done...\")\n",
    "    else:\n",
    "        run_simulation(\n",
    "            metadata,\n",
    "            log=True,\n",
    "            log_train=False,\n",
    "            log_experiment_name=data_collect_experiment_name,\n",
    "            load_policy_experiment_name=policy_experiment_name,\n",
    "            suffix_log_experiment_name=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06588b9",
   "metadata": {},
   "source": [
    "## Statistical test\n",
    "\n",
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "from src.data_classes import Observation, Transition, Step\n",
    "from src.digital_twin.environment_model import EnvironmentModel\n",
    "from src.policy.coma.coma_optimizer import PolicyOptimizerCOMA\n",
    "from src.policy.coma.utils import format_actor_input\n",
    "\n",
    "def format_steps(n_step_start: int, list_steps: List[Step]) -> List[Transition]:\n",
    "    all_observations = [\n",
    "        [\n",
    "            Observation(\n",
    "                n_packets_max=N_PACKETS_MAX,\n",
    "                n_packets_buffer=n_packets_buffer,\n",
    "                data_input=data_input,\n",
    "                ack=ack,\n",
    "                time_step=n_step_start + offset,\n",
    "            )\n",
    "            for ack, data_input, n_packets_buffer in zip(\n",
    "                step.state.channel_ack, step.state.data_generated, step.state.agents_buffer\n",
    "            )\n",
    "        ]\n",
    "        for offset, step in enumerate(list_steps)\n",
    "    ]\n",
    "    return [\n",
    "        Transition(\n",
    "            agents_observations=all_observations[i],\n",
    "            agents_actions=list_steps[i].actions,\n",
    "            agents_next_observations=all_observations[i+1],\n",
    "            agents_rewards=None  # Not useful\n",
    "        )\n",
    "        for i in range(len(all_observations)-1)\n",
    "    ]\n",
    "    \n",
    "\n",
    "def build_test_dataset(n_steps, n_samples=N_TEST_SAMPLES, n_steps_per_sample=N_STEPS_PER_SAMPLE):\n",
    "    # Load data\n",
    "    in_distribution_experiment_name = get_experiment_name_in_distribution_data_collection(n_steps)\n",
    "    in_distribution_episode = Episode.load_episode(in_distribution_experiment_name, \"ep_0\")\n",
    "    out_of_distribution_experiment_name = get_experiment_name_out_of_distribution_data_collection(n_steps)\n",
    "    out_of_distribution_episode = Episode.load_episode(out_of_distribution_experiment_name, \"ep_0\")\n",
    "    \n",
    "    # Sample data per <n_steps_per_sample> batches\n",
    "    max_index_sample = N_STEPS_DATA - (n_steps_per_sample + 1)\n",
    "    in_distribution_sample_indexes = np.random.randint(0, max_index_sample, size=n_samples)\n",
    "    out_of_distribution_sample_indexes = np.random.randint(0, max_index_sample, size=n_samples)\n",
    "    \n",
    "    samples = []\n",
    "    for sample_indexes, episode, label in zip(\n",
    "        [in_distribution_sample_indexes, out_of_distribution_sample_indexes],\n",
    "        [in_distribution_episode, out_of_distribution_episode],\n",
    "        [1, 0]\n",
    "    ):\n",
    "        samples += [\n",
    "            (\n",
    "                format_steps(sample_idx, episode.history[sample_idx:sample_idx+n_steps_per_sample+1]),\n",
    "                label\n",
    "            )\n",
    "            for sample_idx in sample_indexes\n",
    "        ]\n",
    "    \n",
    "    random.shuffle(samples)\n",
    "    \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31c579",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DATASETS dict topology\n",
    "# Dict:\n",
    "# - key : n_steps\n",
    "# - value : List :\n",
    "#           - one entry per sample (either from in or out of distribution)\n",
    "#           - value : Tuple\n",
    "#                     - 1st entry : List[Transition]  (number of entries = N_STEPS_PER_SAMPLE)\n",
    "#                     - 2nd entry : label (0 -> Out-of-distribution ; 1 -> in-distribution)\n",
    "\n",
    "DATASETS=dict()\n",
    "for n_steps in N_STEPS_LIST:\n",
    "    print(f\"Steps={n_steps}\")\n",
    "    DATASETS[n_steps] = build_test_dataset(\n",
    "        n_steps,\n",
    "        n_samples=N_TEST_SAMPLES,\n",
    "        n_steps_per_sample=N_STEPS_PER_SAMPLE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0990d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASETS[10][2*N_TEST_SAMPLES - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b65977",
   "metadata": {},
   "source": [
    "### Get likelihood estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "# logger.setLevel(\"DEBUG\")\n",
    "\n",
    "def get_joint_action_prob(policies, transition: Transition):\n",
    "    p_send_array = np.array([\n",
    "        policy.get_p_transmit(observation)\n",
    "        for policy, observation in zip(policies, transition.agents_observations)\n",
    "    ])\n",
    "    actions_array = np.array(transition.agents_actions)\n",
    "    return np.prod(\n",
    "        (p_send_array * actions_array) +  # probabilities when a packet is sent\n",
    "        ((1 - p_send_array) * (1 - actions_array))  # probabilities when a packet is NOT sent\n",
    "    )\n",
    "\n",
    "def get_selected_submodels_joint_probability(env, transition, selected_transition_submodels):\n",
    "    logger.debug(\"-------------------\")\n",
    "    data_gen = [obs.data_input for obs in transition.agents_observations]\n",
    "    data_gen_next = [obs.data_input for obs in transition.agents_next_observations]\n",
    "    time_step = transition.agents_next_observations[0].time_step\n",
    "    logger.debug(f\"t={time_step} / data_gen={data_gen} / data_gen_next={data_gen_next}\")\n",
    "    \n",
    "    \n",
    "    transition_probabilities = env.get_transition_probabilities(transition)\n",
    "    \n",
    "    logger.debug(transition_probabilities)\n",
    "    \n",
    "    \n",
    "    return np.prod([transition_probabilities[submodel_name] for submodel_name in selected_transition_submodels])\n",
    "    \n",
    "\n",
    "def get_likelihood_samples(\n",
    "    n_model: int,\n",
    "    n_steps: int,\n",
    "    transitions_samples: List[Tuple[List[Transition], int]],\n",
    "    policy_experiment_name: str = None,  # None means action probability is not taken into account,\n",
    "    selected_transition_submodels: list = None,\n",
    "    use_action_probability: bool = True,\n",
    "    n_posterior_samples: int = 1,\n",
    "    frequentist_method: str = \"maximum_a_posteriori\",\n",
    "    frequentist_prior_dirichlet_concentration: float = None\n",
    "):\n",
    "    model_experiment_name = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    \n",
    "    # Load policy and get policies probabilities\n",
    "    if (policy_experiment_name is None) or (not use_action_probability):\n",
    "        if use_action_probability and (policy_experiment_name is None):\n",
    "            logging.warning(\"'policy_experiment_name' set to 'None', ignoring action probabilities in the likelihood !\")\n",
    "        if (not use_action_probability) and (policy_experiment_name is not None):\n",
    "            logging.warning(\"'use_action_probability' set to 'False', ignoring action probabilities in the likelihood !\")\n",
    "        actions_prob = np.ones((len(transitions_samples), len(transitions_samples[0][0])))\n",
    "    else:\n",
    "        policy = PolicyOptimizerCOMA.load(policy_experiment_name)\n",
    "        policies = policy.get_agents_policies()\n",
    "        actions_prob = np.array([\n",
    "            [\n",
    "                get_joint_action_prob(policies, transition)\n",
    "                for transition in transitions\n",
    "            ]\n",
    "            for transitions, _ in transitions_samples\n",
    "        ])\n",
    "    \n",
    "    # Load frequentist model and get likelihoods\n",
    "    frequentist_model = EnvironmentModel.load(model_experiment_name)\n",
    "    if frequentist_prior_dirichlet_concentration is not None:\n",
    "        frequentist_model.update_prior_dirichlet_concentration(frequentist_prior_dirichlet_concentration)\n",
    "    map_env = frequentist_model.init_env(None, None, frequentist_method)\n",
    "    \n",
    "    \n",
    "    map_env_model_prob = []\n",
    "    for transitions, label in transitions_samples:\n",
    "        logger.debug(\"=========================\")\n",
    "        logger.debug(f\"label = {label}\")\n",
    "        \n",
    "        joint_probs = [\n",
    "            get_selected_submodels_joint_probability(\n",
    "                map_env,\n",
    "                transition,\n",
    "                selected_transition_submodels\n",
    "            )\n",
    "            for transition in transitions\n",
    "        ]\n",
    "        \n",
    "        logger.debug(f\"JOINT PROBS = {joint_probs}\")\n",
    "\n",
    "        map_env_model_prob.append(joint_probs)\n",
    "    map_env_model_prob = np.array(map_env_model_prob)\n",
    "    \n",
    "#     map_env_model_prob = np.array([\n",
    "#         [\n",
    "#             get_selected_submodels_joint_probability(\n",
    "#                 map_env,\n",
    "#                 transition,\n",
    "#                 selected_transition_submodels\n",
    "#             )\n",
    "#             for transition in transitions\n",
    "#         ]\n",
    "#         for transitions, _ in transitions_samples\n",
    "#     ])\n",
    "    \n",
    "    map_env_likelihoods = np.prod(  # Multiply probabilities accross the consecutive time steps\n",
    "        actions_prob * map_env_model_prob,  # Overall transition probability per step\n",
    "        axis=1\n",
    "    ).reshape(-1, 1)\n",
    "    \n",
    "    # Load Bayesian model and get likelihoods\n",
    "    bayesian_model = EnvironmentModel.load(model_experiment_name)\n",
    "    posterior_sample_env_model_probability = np.zeros((len(transitions_samples), n_posterior_samples))\n",
    "    posterior_sample_env_likelihoods = np.zeros((len(transitions_samples), n_posterior_samples))\n",
    "    for idx_model_sample in range(n_posterior_samples):\n",
    "        posterior_sample_env = bayesian_model.init_env(None, None, \"posterior_sample\")\n",
    "        posterior_sample_env_model_prob = np.array([\n",
    "            [\n",
    "                get_selected_submodels_joint_probability(\n",
    "                    posterior_sample_env,\n",
    "                    transition,\n",
    "                    selected_transition_submodels\n",
    "                )\n",
    "                for transition in transitions\n",
    "            ]\n",
    "            for transitions, _ in transitions_samples\n",
    "        ])\n",
    "        posterior_sample_env_likelihoods[:, idx_model_sample] = np.prod(  # Multiply probabilities accross the consecutive time steps\n",
    "            actions_prob * posterior_sample_env_model_prob,  # Overall transition probability per step\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return map_env_likelihoods, posterior_sample_env_likelihoods\n",
    "\n",
    "\n",
    "def compute_log_likelihood_estimates(likelihoods, epsilon=EPSILON):\n",
    "    return np.mean(\n",
    "        np.log(likelihoods + epsilon),\n",
    "        axis=1\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "def compute_disagreement_score(likelihoods, epsilon=EPSILON):\n",
    "    # From paper : https://arxiv.org/pdf/1912.05651.pdf\n",
    "    sum_likelihoods = np.sum(likelihoods, axis=1).reshape(-1, 1)\n",
    "    normalized_likelihoods = likelihoods / sum_likelihoods\n",
    "    squared_score = np.power(normalized_likelihoods, 2)\n",
    "    return (1 / np.sum(squared_score, axis=1)).reshape(-1, 1)\n",
    "\n",
    "def compute_log_likelihood_opposite_std(likelihoods, epsilon=EPSILON):\n",
    "    log_likelihoods = np.log(likelihoods + epsilon)\n",
    "    log_likelihood_means = np.mean(log_likelihoods, axis=1).reshape(-1, 1)\n",
    "    log_likelihoods_distances_from_mean = np.power(log_likelihoods - log_likelihood_means, 2)\n",
    "    log_likelihood_variances = np.mean(log_likelihoods_distances_from_mean, axis=1).reshape(-1, 1)\n",
    "    return -np.sqrt(log_likelihood_variances)\n",
    "\n",
    "def compute_log_likelihood_std_v2(ml_likelihoods, posterior_likelihoods, epsilon=EPSILON):\n",
    "    ml_log_likelihoods = np.log(ml_likelihoods + epsilon)\n",
    "    posterior_log_likelihoods = np.log(posterior_likelihoods + epsilon)\n",
    "    log_likelihoods_distances_from_mean = np.power(posterior_log_likelihoods - ml_log_likelihoods, 2)\n",
    "    log_likelihood_variances = np.mean(log_likelihoods_distances_from_mean, axis=1).reshape(-1, 1)\n",
    "    return np.sqrt(log_likelihood_variances)\n",
    "\n",
    "def compute_mutual_information(likelihoods, epsilon=EPSILON):\n",
    "    # TODO: THIS IS NOT CORRECT !!!\n",
    "    marginal_likelihood = np.mean(likelihoods, axis=1).reshape(-1, 1)\n",
    "    kl_distance_to_marginal_likelihood = np.log((likelihoods + epsilon) / (marginal_likelihood + epsilon))\n",
    "    return np.mean(kl_distance_to_marginal_likelihood, axis=1).reshape(-1, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53ff89",
   "metadata": {},
   "source": [
    "### Get test estimators (get soft prediction i.e. likelihoods only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_TRANSITION_SUBMODELS = [\n",
    "    \"data_gen_cluster_1\", #\"data_gen_cluster_2\"\n",
    "]\n",
    "N_POSTERIOR_SAMPLES = 1000  # Number of posterior sampled models\n",
    "FREQUENTIST_MODEL_METHOD = \"maximum_a_posteriori\"\n",
    "FREQUENTIST_PRIOR_DIRICHLET_CONCENTRATION = PRIOR_DIRICHLET_CONCETRATION_MAP\n",
    "# \"maximum_a_posteriori\" is similar to \"maximum_likelihood\" if prior concentration is low\n",
    "\n",
    "# Select which function to use for estimation of epistemic uncertainty\n",
    "LOG_LIKELIHOOD_THRESHOLD_RANGE = (np.log(EPSILON), np.log(1 + EPSILON))\n",
    "\n",
    "# Standard deviation of posterior models using ensemble predictor as mean\n",
    "# Note: technically when using the ML as mean we are not using the ensemble predictor but this is quite similar for small prior concentration\n",
    "# BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC = compute_log_likelihood_std_v2\n",
    "# BAYESIAN_ESTIMATOR_THRESHOLD_RANGE = (np.log(1 + EPSILON), -np.log(EPSILON))\n",
    "\n",
    "# Opposite standard deviation of posterior models using posterior sample mean\n",
    "BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC = compute_log_likelihood_opposite_std\n",
    "BAYESIAN_ESTIMATOR_THRESHOLD_RANGE = (np.log(EPSILON), 0)\n",
    "\n",
    "# Disagreement score of posterior models\n",
    "# BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC = compute_disagreement_score\n",
    "# BAYESIAN_ESTIMATOR_THRESHOLD_RANGE = (1, N_POSTERIOR_SAMPLES)\n",
    "\n",
    "# Average log-likelihood of posterior models\n",
    "# BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC = compute_log_likelihood_estimates\n",
    "# BAYESIAN_ESTIMATOR_THRESHOLD_RANGE = (np.log(EPSILON), np.log(1 + EPSILON))\n",
    "\n",
    "# Multual information\n",
    "# BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC = compute_mutual_information\n",
    "# BAYESIAN_ESTIMATOR_THRESHOLD_RANGE = (-np.log(1 + (1 / (N_POSTERIOR_SAMPLES * EPSILON))), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f77ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# Estimators dict topology\n",
    "# Dict:\n",
    "# - key : n_steps\n",
    "# - value : List :\n",
    "#           - one entry per n_model\n",
    "#           - value : Array[2 * N_STEPS_DATA ; 3]\n",
    "#                     - Column 1 : map_env_likelihoods\n",
    "#                     - Column 2 : posterior_env_likelihoods or scores\n",
    "#                     - Column 3 : labels\n",
    "ESTIMATORS = dict()\n",
    "\n",
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    print(f\"Steps={n_steps} / Model={n_model}\")\n",
    "    map_env_likelihoods, posterior_env_likelihoods = get_likelihood_samples(\n",
    "        n_model,\n",
    "        n_steps,\n",
    "        DATASETS[n_steps],\n",
    "        policy_experiment_name=None, # DO NOT TAKE ACTION PROB INTO ACCOUNT, OTHERWISE SET TO \"SELECTED_POLICY[n_steps]\",\n",
    "        selected_transition_submodels=SELECTED_TRANSITION_SUBMODELS,\n",
    "        use_action_probability=False,\n",
    "        n_posterior_samples=N_POSTERIOR_SAMPLES,\n",
    "        frequentist_method=FREQUENTIST_MODEL_METHOD,\n",
    "        frequentist_prior_dirichlet_concentration=FREQUENTIST_PRIOR_DIRICHLET_CONCENTRATION\n",
    "    )\n",
    "    map_env_estimator = compute_log_likelihood_estimates(map_env_likelihoods)\n",
    "#         posterior_env_estimator = BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC(ml_env_likelihoods, posterior_env_likelihoods)\n",
    "    posterior_env_estimator = BAYESIAN_EPISTEMIC_UNCERTAINTY_ESTIMATOR_FUNC(posterior_env_likelihoods)\n",
    "    labels = np.array([label for _, label in DATASETS[n_steps]]).reshape(-1, 1)\n",
    "    if n_steps not in ESTIMATORS.keys():\n",
    "        ESTIMATORS[n_steps] = []\n",
    "    ESTIMATORS[n_steps].append(np.c_[\n",
    "        map_env_estimator,\n",
    "        posterior_env_estimator,\n",
    "        labels\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATORS[20][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5ca06",
   "metadata": {},
   "source": [
    "## Plot ROC curves\n",
    "\n",
    "### Get ROC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa25cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(log_likelihoods, labels, threshold, normalize=False):\n",
    "    _label_map = {\n",
    "        (0, 0): \"TN\",\n",
    "        (0, 1): \"FN\",\n",
    "        (1, 0): \"FP\",\n",
    "        (1, 1): \"TP\",\n",
    "    }\n",
    "    preds = np.where(log_likelihoods >= threshold, 1, 0)\n",
    "    values, counts = np.unique(np.c_[preds, labels], return_counts=True, axis=0)\n",
    "    if normalize:\n",
    "        counts = counts.astype(np.float32) / len(labels)\n",
    "    out = dict(\n",
    "        TN=0, FN=0, FP=0, TP=0\n",
    "    )\n",
    "    out.update({\n",
    "        _label_map[tuple(pred_label_pair)]: nb_occurences\n",
    "        for pred_label_pair, nb_occurences in zip(values, counts)\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def get_roc_data(estimator, labels, threshold_range, num_points=100):\n",
    "    thresholds = np.linspace(threshold_range[0], threshold_range[1], num_points)\n",
    "    false_positive_rate = []\n",
    "    true_positive_rate = []\n",
    "    for threshold in thresholds:\n",
    "        cm = get_confusion_matrix(estimator, labels, threshold)\n",
    "        false_positive_rate.append(cm[\"FP\"] / (cm[\"FP\"] + cm[\"TN\"]))\n",
    "        true_positive_rate.append(cm[\"TP\"] / (cm[\"TP\"] + cm[\"FN\"]))\n",
    "    return false_positive_rate, true_positive_rate, thresholds\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3f5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all roc_data estimators\n",
    "NUM_POINTS = 10000\n",
    "\n",
    "ROC_DATA_MAP = dict()\n",
    "ROC_DATA_POSTERIOR = dict()\n",
    "\n",
    "for n_steps, estimators_list in ESTIMATORS.items():\n",
    "    ROC_DATA_MAP[n_steps] = {\"fpr\": None, \"tpr\": None}\n",
    "    ROC_DATA_POSTERIOR[n_steps] = {\"fpr\": None, \"tpr\": None}\n",
    "    for n_model, estimator in enumerate(estimators_list):\n",
    "        print(f\"Model={n_model} / Steps={n_steps}\")\n",
    "        fpr_ml, tpr_ml, _ = get_roc_data(\n",
    "            estimator[:, 0],\n",
    "            estimator[:, 2],\n",
    "            LOG_LIKELIHOOD_THRESHOLD_RANGE,\n",
    "            num_points=NUM_POINTS\n",
    "        )\n",
    "        # ML\n",
    "        if ROC_DATA_MAP[n_steps][\"fpr\"] is None:\n",
    "            ROC_DATA_MAP[n_steps][\"fpr\"] = fpr_ml\n",
    "            ROC_DATA_MAP[n_steps][\"tpr\"] = tpr_ml\n",
    "        else:\n",
    "            ROC_DATA_MAP[n_steps][\"fpr\"] = np.c_[ROC_DATA_MAP[n_steps][\"fpr\"], fpr_ml]\n",
    "            ROC_DATA_MAP[n_steps][\"tpr\"] = np.c_[ROC_DATA_MAP[n_steps][\"tpr\"], tpr_ml]\n",
    "        \n",
    "        # Posterior\n",
    "        fpr_posterior, tpr_posterior, _ = get_roc_data(\n",
    "            estimator[:, 1],\n",
    "            estimator[:, 2],\n",
    "            BAYESIAN_ESTIMATOR_THRESHOLD_RANGE,\n",
    "            num_points=NUM_POINTS\n",
    "        )\n",
    "        if ROC_DATA_POSTERIOR[n_steps][\"fpr\"] is None:\n",
    "            ROC_DATA_POSTERIOR[n_steps][\"fpr\"] = fpr_posterior\n",
    "            ROC_DATA_POSTERIOR[n_steps][\"tpr\"] = tpr_posterior\n",
    "        else:\n",
    "            ROC_DATA_POSTERIOR[n_steps][\"fpr\"] = np.c_[\n",
    "                ROC_DATA_POSTERIOR[n_steps][\"fpr\"],\n",
    "                fpr_posterior\n",
    "            ]\n",
    "            ROC_DATA_POSTERIOR[n_steps][\"tpr\"] = np.c_[\n",
    "                ROC_DATA_POSTERIOR[n_steps][\"tpr\"],\n",
    "                tpr_posterior\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf2aea",
   "metadata": {},
   "source": [
    "### Plot ROC curves with standard deviation across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_with_errors(\n",
    "    ax: plt.Axes,\n",
    "    roc_data,\n",
    "    lineplot_kwargs: dict = None\n",
    "):\n",
    "    rates_mean = []\n",
    "    rates_low = []\n",
    "    rates_high = []\n",
    "    for rate in (roc_data[\"fpr\"], roc_data[\"tpr\"]):\n",
    "        rate_mean = np.mean(rate, axis=1)\n",
    "        rate_std = np.std(rate, axis=1)\n",
    "        rates_mean.append(rate_mean)\n",
    "        rates_low.append(rate_mean - (1.96 * rate_std))\n",
    "        rates_high.append(rate_mean + (1.96 * rate_std))\n",
    "    \n",
    "    # Axis formatting\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_xlim([-0.05, 1.05])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    \n",
    "    # Plot ROC\n",
    "    ax.plot(rates_mean[0], rates_mean[1], **lineplot_kwargs)  # mean val\n",
    "    ax.fill_between(rates_mean[0], rates_low[1], rates_high[1], alpha=.20, **lineplot_kwargs)\n",
    "    \n",
    "    # Plot random pred diagonal\n",
    "    ax.plot([0, 1], [0, 1], color=\"black\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_n_steps = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve_with_errors(\n",
    "    ax,\n",
    "    ROC_DATA_MAP[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve_with_errors(\n",
    "    ax,\n",
    "    ROC_DATA_POSTERIOR[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb0109",
   "metadata": {},
   "source": [
    "### Plot multiple mean ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeefa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from src.view.utils import add_annotation\n",
    "\n",
    "def plot_multiple_mean_roc_curves(\n",
    "    ax: plt.Axes,\n",
    "    all_roc_data: Dict[int, dict],\n",
    "    n_steps_models_to_plot: List[int],\n",
    "    all_lineplot_kwargs: Dict[int, dict] = None,\n",
    "    label_annotation: str = \"\",\n",
    "    y_pos_text_annotation: float = 0.4,\n",
    "    y_pos_arrow_annotation: float = 0.7\n",
    "):\n",
    "    all_lineplot_kwargs = all_lineplot_kwargs or dict()\n",
    "    \n",
    "    # Get means\n",
    "    all_rates_means = []\n",
    "    for n_steps in n_steps_models_to_plot:\n",
    "        roc_data = all_roc_data[n_steps]\n",
    "        fpr_mean = np.mean(roc_data[\"fpr\"], axis=1)\n",
    "        tpr_mean = np.mean(roc_data[\"tpr\"], axis=1)\n",
    "        all_rates_means.append((fpr_mean, tpr_mean))\n",
    "    \n",
    "    # Plot formatting\n",
    "    fig, ax = subplot\n",
    "    fig.set_size_inches(10, 10)\n",
    "    min_ax_lim = -0.05\n",
    "    max_ax_lim = 1.05\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_xlim([min_ax_lim, max_ax_lim])\n",
    "    ax.set_ylim([min_ax_lim, max_ax_lim])\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.xaxis.label.set_size(24)\n",
    "    ax.yaxis.label.set_size(24)\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    for n_steps, rates_mean in zip(n_steps_models_to_plot, all_rates_means):\n",
    "        ax.plot(rates_mean[0], rates_mean[1], **all_lineplot_kwargs.get(n_steps, dict()))\n",
    "    \n",
    "    # Print fpr at tpr=0.8\n",
    "    indicator_tpr_value = 0.8\n",
    "    for n_steps, rates_mean in zip(n_steps_models_to_plot, all_rates_means):\n",
    "        tpr_arr = np.array(rates_mean[1])\n",
    "        idx_indicator = np.argmax(tpr_arr <= indicator_tpr_value)\n",
    "        indicator_tpr = rates_mean[1][idx_indicator]\n",
    "        indicator_fpr = rates_mean[0][idx_indicator]\n",
    "        print(f\"Indicator point for n_steps={n_steps}: {(indicator_fpr, indicator_tpr)}\")\n",
    "    \n",
    "    # Plot random pred diagonal\n",
    "    ax.plot([0, 1], [0, 1], color=\"black\", linestyle=\":\")\n",
    "    \n",
    "    # Add anotations\n",
    "    x_pos_text_annotation = 0.75\n",
    "    all_pos_arrow = []\n",
    "    for rates_mean in all_rates_means:\n",
    "        tpr_arr = np.array(rates_mean[1])\n",
    "        idx_pos = np.argmax(tpr_arr <= y_pos_arrow_annotation)\n",
    "        all_pos_arrow.append(\n",
    "            (rates_mean[0][idx_pos], rates_mean[1][idx_pos])\n",
    "        )\n",
    "    \n",
    "    for pos_arrow in all_pos_arrow:\n",
    "        add_annotation(\n",
    "            ax,\n",
    "            label_annotation,\n",
    "            pos_arrow,\n",
    "            (x_pos_text_annotation, y_pos_text_annotation),\n",
    "            arrow_kwargs=dict(\n",
    "                mutation_scale=15,\n",
    "                linewidth=1\n",
    "            ),\n",
    "            text_kwargs=dict(\n",
    "                fontsize=28\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_to_plot = [20, 50]\n",
    "\n",
    "subplot = plt.subplots()\n",
    "\n",
    "plot_multiple_mean_roc_curves(\n",
    "    subplot,\n",
    "    ROC_DATA_MAP,\n",
    "    n_steps_to_plot,\n",
    "    all_lineplot_kwargs={\n",
    "        n_steps_to_plot[0]: {\"color\": \"green\", \"label\": \"MAP Model\", \"linewidth\": 2.5},\n",
    "        n_steps_to_plot[1]: {\"color\": \"green\", \"label\": \"MAP Model\", \"linewidth\": 2.5, \"linestyle\":\"-.\"},\n",
    "    },\n",
    "    label_annotation=\"Frequentist\",\n",
    "    y_pos_text_annotation=0.5,\n",
    "    y_pos_arrow_annotation=0.8\n",
    ")\n",
    "\n",
    "\n",
    "plot_multiple_mean_roc_curves(\n",
    "    subplot,\n",
    "    ROC_DATA_POSTERIOR,\n",
    "    n_steps_to_plot,\n",
    "    all_lineplot_kwargs={\n",
    "        n_steps_to_plot[0]: {\"color\": \"orange\", \"label\": \"Posterior Sampled Model\", \"linewidth\": 2.5},\n",
    "        n_steps_to_plot[1]: {\"color\": \"orange\", \"label\": \"Posterior Sampled Model\", \"linewidth\": 2.5, \"linestyle\":\"-.\"},\n",
    "    },\n",
    "    label_annotation=\"Bayesian\",\n",
    "    y_pos_text_annotation=0.3,\n",
    "    y_pos_arrow_annotation=0.65\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50bd5ad",
   "metadata": {},
   "source": [
    "## Annex\n",
    "\n",
    "### Check likelihood mean and std for MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea12b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_likelihood_mean_and_std_for_each_ground_truth_dynamics(n_model, n_step, n_steps_per_sample):\n",
    "    _cluster_joint_states = [(0,0), (0,1), (1,0), (1,1)]\n",
    "    _cluster_joint_states_encoded = [\"0,0\", \"0,1\", \"1,0\", \"1,1\"]\n",
    "    \n",
    "    # Get ground truth dynamics for cluster 1\n",
    "    normal_dynamics_cluster = np.array([\n",
    "        DEFAULT_JOINT_DISTRIBUTION[joint_state_data_gen]\n",
    "        for joint_state_data_gen in _cluster_joint_states_encoded\n",
    "    ])\n",
    "    abnormal_dynamics_cluster = np.array([\n",
    "        PERTURBED_DEFAULT_JOINT_DISTRIBUTION[joint_state_data_gen]\n",
    "        for joint_state_data_gen in _cluster_joint_states_encoded\n",
    "    ])\n",
    "    \n",
    "    # Get model cluster 1 dynamics\n",
    "    model_experiment_name = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    model = EnvironmentModel.load(model_experiment_name)\n",
    "    model.update_prior_dirichlet_concentration(PRIOR_DIRICHLET_CONCETRATION_MAP)\n",
    "    env = model.init_env(None, None, \"maximum_a_posteriori\")\n",
    "    p_map_cluster = env.data_gen.transition._indexed_probabilities_maps[\"cluster_1\"]\n",
    "    model_dynamics_cluster = np.array([\n",
    "        p_map_cluster._dict_default_joint_distribution[joint_state_data_gen]\n",
    "        for joint_state_data_gen in _cluster_joint_states\n",
    "    ])\n",
    "    \n",
    "    # Mean\n",
    "    normal_mean_likelihood = np.sum(normal_dynamics_cluster * model_dynamics_cluster)\n",
    "    abnormal_mean_likelihood = np.sum(abnormal_dynamics_cluster * model_dynamics_cluster)\n",
    "    \n",
    "    # Var\n",
    "    normal_var_likelihood = np.sum(\n",
    "        normal_dynamics_cluster * np.power(\n",
    "            model_dynamics_cluster - normal_mean_likelihood,\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "    abnormal_var_likelihood = np.sum(\n",
    "        abnormal_dynamics_cluster * np.power(\n",
    "            model_dynamics_cluster - abnormal_mean_likelihood,\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Scale for consecutive time steps likelihoods (this is only true if data gen is independent across time steps)\n",
    "    normal_mean_likelihood *= n_steps_per_sample\n",
    "    abnormal_mean_likelihood *= n_steps_per_sample\n",
    "    normal_var_likelihood *= n_steps_per_sample\n",
    "    abnormal_var_likelihood *= n_steps_per_sample\n",
    "    \n",
    "    return {\n",
    "        \"normal\": {\"mean\": normal_mean_likelihood, \"std\": normal_var_likelihood**(1/2)},\n",
    "        \"abnormal\": {\"mean\": abnormal_mean_likelihood, \"std\": abnormal_var_likelihood**(1/2)},\n",
    "    }\n",
    "\n",
    "def plot_average_likelihoods_clt_gaussians(ax, n_model, n_steps, n_steps_per_sample, epsilon=0):\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    likelihood_means_stds = get_model_likelihood_mean_and_std_for_each_ground_truth_dynamics(\n",
    "        n_model, n_steps, n_steps_per_sample, epsilon=epsilon\n",
    "    )\n",
    "    n_points = 1000\n",
    "    max_std = max(ll_means_stds[\"normal\"][\"std\"], ll_means_stds[\"abnormal\"][\"std\"])\n",
    "    min_mean = min(ll_means_stds[\"normal\"][\"mean\"], ll_means_stds[\"abnormal\"][\"mean\"])\n",
    "    max_mean = max(ll_means_stds[\"normal\"][\"mean\"], ll_means_stds[\"abnormal\"][\"mean\"])\n",
    "    x_range = (-3*max_std + min_mean, 3*max_std + max_mean)\n",
    "    x_space = np.linspace(*x_range, n_points)\n",
    "    \n",
    "    normal_clt_std = likelihood_means_stds[\"normal\"][\"std\"] / 1 #np.sqrt(N_TEST_SAMPLES)\n",
    "    ax.plot(\n",
    "        x_space,\n",
    "        stats.norm.pdf(x_space, likelihood_means_stds[\"normal\"][\"mean\"], normal_clt_std),\n",
    "        color=\"green\"\n",
    "    )\n",
    "    \n",
    "    abnormal_clt_std = likelihood_means_stds[\"abnormal\"][\"std\"] / 1 #np.sqrt(N_TEST_SAMPLES)\n",
    "    ax.plot(\n",
    "        x_space,\n",
    "        stats.norm.pdf(x_space, likelihood_means_stds[\"abnormal\"][\"mean\"], abnormal_clt_std),\n",
    "        color=\"red\"\n",
    "    )\n",
    "    \n",
    "\n",
    "n_steps = 10\n",
    "n_models = 50\n",
    "\n",
    "subplots = plt.subplots((n_models // 5) + 1, 5)\n",
    "fig, axs = subplots\n",
    "fig.set_size_inches(15, 10)\n",
    "axs = axs.flatten()\n",
    "\n",
    "n_steps_per_sample = N_STEPS_PER_SAMPLE\n",
    "for n_model in range(n_models):\n",
    "    plot_average_log_likelihoods_clt_gaussians(axs[n_model], n_model, n_steps, n_steps_per_sample, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43265a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be1f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded828c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual_info\n",
    "select_n_steps = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893465f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disagreement\n",
    "select_n_steps = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d77b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "select_n_steps = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ae7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var\n",
    "select_n_steps = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e994ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "select_n_steps = 10\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LL\n",
    "select_n_steps = 10\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var\n",
    "select_n_steps = 10\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c697e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual_info\n",
    "select_n_steps = 10\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_ml[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"green\", \"label\": \"Maximum Likelihood Model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plot_roc_curve(\n",
    "    ax,\n",
    "    roc_data_posterior[select_n_steps],\n",
    "    lineplot_kwargs={\n",
    "        \"color\": \"orange\", \"label\": \"Posterior Sampled Model\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a19ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt-mpr-channel",
   "language": "python",
   "name": "dt-mpr-channel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
