{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e42beb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Paper study - policy optimization\n",
    "\n",
    "## Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878676a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "\n",
    "from settings import Metadata, EnvMetadata, ExperimentPhase, PROJECT_FOLDER, DATA_FOLDER, setup_matplotlib_config\n",
    "from bin.main import run_simulation\n",
    "from src.utils import lineplot_ci\n",
    "from src.data_classes import Episode\n",
    "from src.environment.channel.utils import create_K_MPR_matrix\n",
    "from src.view.plot import plot_episode, plot_validation_metrics, plot_per_step_metrics, plot_validation_metrics_barplots, \\\n",
    "    plot_validation_metrics_per_n_steps_model_learning, \\\n",
    "    plot_p_transmit_aac, plot_critic_value_aac, plot_training_actor_critic, plot_coma_actors_critic, \\\n",
    "    plot_tdma_actors_intermediary_probabilities\n",
    "from src.view.plot_model import plot_dirichlet_data_generation, plot_dirichlet_mpr_channel\n",
    "from src.view.metrics import get_experiment_throughput, get_experiment_fairness, experiment_throughput_mean_dev, get_return, \\\n",
    "    get_buffer_info, get_channel_collisions, get_aac_info, get_training_info, get_state_distribution,  \\\n",
    "    get_coma_info, get_loss, get_critic_training_info, get_gradients_info, estimate_Q_value_COMA\n",
    "\n",
    "\n",
    "setup_matplotlib_config()\n",
    "\n",
    "root = logging.getLogger()\n",
    "if root.handlers:\n",
    "    for handler in root.handlers:\n",
    "        root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s %(message)s',level=logging.INFO)\n",
    "\n",
    "def does_experiment_exist(experiment_name):\n",
    "    return os.path.isdir(os.path.join(PROJECT_FOLDER, DATA_FOLDER, experiment_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63073ada",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163b225",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from experimental_setup import *\n",
    "\n",
    "# Log\n",
    "EXPERIMENT_NAME_PREFIX = \"paper_policy_opt\"\n",
    "\n",
    "# Print\n",
    "_JOINT_SINGLE_PACKET_GENERATED_PROBABILITY = DEFAULT_JOINT_DISTRIBUTION[\"1,0\"]\n",
    "_MARGINAL_DATA_GEN_PROBABILITY = _JOINT_SINGLE_PACKET_GENERATED_PROBABILITY / (1 + _JOINT_SINGLE_PACKET_GENERATED_PROBABILITY)\n",
    "print(f\"Marginal packet generation probability per agent : {_MARGINAL_DATA_GEN_PROBABILITY}\")\n",
    "print(f\"Joint node distribution : {DEFAULT_JOINT_DISTRIBUTION}\")\n",
    "print(f\"MPR Matrix : {MPR_MATRIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d78e59",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TDMA baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2f676",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def get_metadata_tdma(agents_conf):\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"digital_twin_class\": \"DigitalTwinPolicyPassthrough\",\n",
    "            \"digital_twin_kwargs\": {},\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerTDMA\",\n",
    "            \"policy_optimizer_kwargs\": {\n",
    "                \"tdma_agents_config\": agents_conf\n",
    "            },\n",
    "        }\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670161a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4 slots frames (no prior info on data generation process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed857bda",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TDMA_4_SLOTS_AGENTS_CONF = [\n",
    "    # Agents 1\n",
    "    {\n",
    "        \"frame_length\": TDMA_FRAME_LENGTH,\n",
    "        \"transmission_slot\": 0,\n",
    "        \"transmission_probability\": 1,\n",
    "    },\n",
    "    # Agent 2\n",
    "    {\n",
    "        \"frame_length\": TDMA_FRAME_LENGTH,\n",
    "        \"transmission_slot\": 1,\n",
    "        \"transmission_probability\": 1,\n",
    "    },\n",
    "    # Agents 3\n",
    "    {\n",
    "        \"frame_length\": TDMA_FRAME_LENGTH,\n",
    "        \"transmission_slot\": 2,\n",
    "        \"transmission_probability\": 1,\n",
    "    },\n",
    "    # Agent 4 \n",
    "    {\n",
    "        \"frame_length\": TDMA_FRAME_LENGTH,\n",
    "        \"transmission_slot\": 3,\n",
    "        \"transmission_probability\": 1,\n",
    "    }\n",
    "]\n",
    "\n",
    "metadata_tdma_4_slots = get_metadata_tdma(TDMA_4_SLOTS_AGENTS_CONF)\n",
    "# experiment_name_tdma_4_slots = f\"{EXPERIMENT_NAME_PREFIX}_tdma_4_slots_01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a4cf1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2 slots frames (prior info on data generation joint probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cae404",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TDMA_2_SLOTS_AGENTS_CONF = [\n",
    "    # Agents 1 and 2 in slot 1\n",
    "    {\n",
    "        \"frame_length\": 2,\n",
    "        \"transmission_slot\": 0,\n",
    "        \"transmission_probability\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"frame_length\": 2,\n",
    "        \"transmission_slot\": 0,\n",
    "        \"transmission_probability\": 1,\n",
    "    },\n",
    "    # Agents 3 and 4 in slot 2\n",
    "    {\n",
    "        \"frame_length\": 2,\n",
    "        \"transmission_slot\": 1,\n",
    "        \"transmission_probability\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"frame_length\": 2,\n",
    "        \"transmission_slot\": 1,\n",
    "        \"transmission_probability\": 1,\n",
    "    }\n",
    "]\n",
    "\n",
    "metadata_tdma_2_slots = get_metadata_tdma(TDMA_2_SLOTS_AGENTS_CONF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622453a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model-Free baseline (COMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ce33c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_TRAININGS_MODEL_FREE = 5\n",
    "EXPERIMENT_NAME_TEST_EPISODES_MODEL_FREE = f\"{EXPERIMENT_NAME_PREFIX}_model_free_01_test_episodes\"\n",
    "\n",
    "def get_metadata_model_free():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinPolicyPassthrough\",\n",
    "            \"digital_twin_kwargs\": {},\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": N_EPISODES_TRAINING_POLICY,\n",
    "            \"train_policy_max_steps\": N_STEPS_TRAINING_POLICY,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA,\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_model_free(n_training):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_model_free_01_n_training_{n_training}\"\n",
    "\n",
    "metadata_model_free = get_metadata_model_free()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da1b56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0f20e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "run_simulation(\n",
    "    metadata_tdma_4_slots,\n",
    "    log=True,\n",
    "    log_train=False,\n",
    "    log_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_FREE,\n",
    "    suffix_log_experiment_name=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2606561",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train and test model-free policy multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245f15f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "for n_training in range(N_TRAININGS_MODEL_FREE):\n",
    "    run_simulation(\n",
    "        metadata_model_free,\n",
    "        log=True,\n",
    "        log_train=True,\n",
    "        log_experiment_name=get_experiment_name_model_free(n_training),\n",
    "        suffix_log_experiment_name=False,\n",
    "        load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_FREE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30808004",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test TDMA baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f97d14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate TDMA baselines using the same test episodes\n",
    "\n",
    "# TDMA 4 slots\n",
    "experiment_name_tdma_4_slots_on_mf_test = run_simulation(\n",
    "    metadata_tdma_4_slots,\n",
    "    log=True,\n",
    "    log_train=False,\n",
    "    log_experiment_name=f\"{EXPERIMENT_NAME_PREFIX}_tdma_4_slots_on_mf_test\",\n",
    "    suffix_log_experiment_name=True,\n",
    "    load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_FREE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# TDMA 2 slots\n",
    "experiment_name_tdma_2_slots_on_mf_test = run_simulation(\n",
    "    metadata_tdma_2_slots,\n",
    "    log=True,\n",
    "    log_train=False,\n",
    "    log_experiment_name=f\"{EXPERIMENT_NAME_PREFIX}_tdma_2_slots_on_mf_test\",\n",
    "    suffix_log_experiment_name=True,\n",
    "    load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_FREE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f885c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment_name_tdma_4_slots_on_mf_test = \"paper_policy_opt_tdma_4_slots_on_mf_test_202208241633\"\n",
    "experiment_name_tdma_2_slots_on_mf_test = \"paper_policy_opt_tdma_2_slots_on_mf_test_202208241633\"\n",
    "# Load model free test episodes for each training\n",
    "experiments_model_free_on_mf_test = []\n",
    "for n_training in range(N_TRAININGS_MODEL_FREE):\n",
    "    experiments_model_free_on_mf_test.append(\n",
    "        Episode.load_experiment(\n",
    "            get_experiment_name_model_free(n_training),\n",
    "            experiment_phase=ExperimentPhase.TEST_POLICY\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Load TDMA test episodes\n",
    "experiment_tdma_4_slots_on_mf_test = Episode.load_experiment(\n",
    "    experiment_name_tdma_4_slots_on_mf_test,\n",
    "    experiment_phase=ExperimentPhase.TEST_POLICY\n",
    ")\n",
    "experiment_tdma_2_slots_on_mf_test = Episode.load_experiment(\n",
    "    experiment_name_tdma_2_slots_on_mf_test,\n",
    "    experiment_phase=ExperimentPhase.TEST_POLICY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd85edf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot model-free baseline against TDMA baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93c64f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "subplots = plt.subplots(1, 4)\n",
    "\n",
    "plot_validation_metrics_barplots(\n",
    "        subplots,\n",
    "        0,\n",
    "        [experiment_tdma_4_slots_on_mf_test],\n",
    "        N_STEPS_RETURN_PLOT,\n",
    "        RETURN_DISCOUNT,\n",
    "        barplot_kwargs={\"color\": \"tab:red\"},\n",
    "        n_packets_max=N_PACKETS_MAX,\n",
    "        max_throughput=1.2\n",
    ")\n",
    "\n",
    "plot_validation_metrics_barplots(\n",
    "        subplots,\n",
    "        1,\n",
    "        [experiment_tdma_2_slots_on_mf_test],\n",
    "        N_STEPS_RETURN_PLOT,\n",
    "        RETURN_DISCOUNT,\n",
    "        barplot_kwargs={\"color\": \"tab:green\"},\n",
    "        n_packets_max=N_PACKETS_MAX,\n",
    "        max_throughput=1.2\n",
    ")\n",
    "\n",
    "plot_validation_metrics_barplots(\n",
    "        subplots,\n",
    "        2,\n",
    "        experiments_model_free_on_mf_test,\n",
    "        N_STEPS_RETURN_PLOT,\n",
    "        RETURN_DISCOUNT,\n",
    "        barplot_kwargs={\"color\": \"tab:blue\"},\n",
    "        n_packets_max=N_PACKETS_MAX,\n",
    "        max_throughput=1.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf8dc4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train multiple models on random exploration policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c21b53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_VERSION = \"08\"  # Version of runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670045f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed38f24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "LOG_POLICY_TRAINING = False  # Logging training can take a lot of disk\n",
    "\n",
    "\n",
    "def get_metadata_model_based_test_episodes():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"digital_twin_class\": \"DigitalTwinPolicyPassthrough\",\n",
    "            \"digital_twin_kwargs\": {},\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerTDMA\",\n",
    "            \"policy_optimizer_kwargs\": {\n",
    "                \"tdma_agents_config\": TDMA_4_SLOTS_AGENTS_CONF\n",
    "            },\n",
    "        }\n",
    "    })\n",
    "\n",
    "EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED = f\"{EXPERIMENT_NAME_PREFIX}_model_based_test_episodes_{EXPERIMENT_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4e863",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "metadata_test_episodes = get_metadata_model_based_test_episodes()\n",
    "run_simulation(\n",
    "    metadata_test_episodes,\n",
    "    log=True,\n",
    "    log_train=False,\n",
    "    log_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "    suffix_log_experiment_name=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67204e6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96bf50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_MODELS_PER_STEP = 50\n",
    "N_STEPS_INTERVAL = 1\n",
    "N_STEPS_MAX = 20\n",
    "N_STEPS_RANGE = list(range(0, N_STEPS_MAX+N_STEPS_INTERVAL, N_STEPS_INTERVAL))\n",
    "RANGE_MODEL_STEPS = list(product(range(N_MODELS_PER_STEP), N_STEPS_RANGE))\n",
    "\n",
    "def get_metadata_model_learning():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": {\n",
    "            **ENV_METADATA,\n",
    "            \"test_n_episodes\": 0,\n",
    "        },\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"posterior_sample\",\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 1,\n",
    "            \"train_model_max_steps\": N_STEPS_MAX,\n",
    "            \"train_policy_n_episodes\": 0,\n",
    "            \"train_policy_max_steps\": 0,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "def get_model_learning_experiment_name(n_model, n_steps, output_experiment_name_prefix=False):\n",
    "    n_steps_suffix = \"\" if output_experiment_name_prefix else f\"_n_steps_{n_steps}\"\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_model_learning_random_{EXPERIMENT_VERSION}_n_model_{n_model}{n_steps_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c8184",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "for n_model in range(N_MODELS_PER_STEP):\n",
    "    skip_model = False\n",
    "    metadata_model_learning = get_metadata_model_learning()\n",
    "    experiment_name_model_learning_prefix = get_model_learning_experiment_name(n_model, None, output_experiment_name_prefix=True)\n",
    "    for n_steps in range(N_STEPS_MAX):\n",
    "        experiment_name = f\"{experiment_name_model_learning_prefix}_n_steps_{n_steps}\"\n",
    "        if does_experiment_exist(experiment_name) and (not skip_model):\n",
    "            skip_model = True\n",
    "            print(f\"\"\"\n",
    "                Experiment '{experiment_name}' already done... \n",
    "                Skipping models with prefix '{experiment_name_model_learning_prefix}' !\n",
    "            \"\"\")\n",
    "    if not skip_model:\n",
    "        run_simulation(\n",
    "            metadata_model_learning,\n",
    "            log=True,\n",
    "            log_train=True,\n",
    "            log_experiment_name=experiment_name_model_learning_prefix,\n",
    "            suffix_log_experiment_name=False,\n",
    "            log_model_at_each_step=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5384b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episode_max_steps = Episode.load_experiment(\n",
    "    get_model_learning_experiment_name(0, None, output_experiment_name_prefix=True),\n",
    "    experiment_phase=ExperimentPhase.TRAIN_MODEL\n",
    ")[0]\n",
    "plot_dirichlet_data_generation(\n",
    "    episode_max_steps,\n",
    "    \"cluster_1\",\n",
    "    prior_dirichlet_concentration=PRIOR_DIRICHLET_CONCETRATION,\n",
    "    prior_dirichlet_concentration_map=PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "    n_steps_per_plot=N_STEPS_INTERVAL,\n",
    "    true_transition_probability={\"\": DEFAULT_JOINT_DISTRIBUTION},\n",
    "    x_axis_range=[0, 1],\n",
    "    x_axis_step=0.001\n",
    ")\n",
    "plot_dirichlet_data_generation(\n",
    "    episode_max_steps,\n",
    "    \"cluster_2\",\n",
    "    prior_dirichlet_concentration=PRIOR_DIRICHLET_CONCETRATION,\n",
    "    prior_dirichlet_concentration_map=PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "    n_steps_per_plot=N_STEPS_INTERVAL,\n",
    "    true_transition_probability={\"\": DEFAULT_JOINT_DISTRIBUTION},\n",
    "    x_axis_range=[0, 1],\n",
    "    x_axis_step=0.001\n",
    ")\n",
    "plot_dirichlet_mpr_channel(\n",
    "    episode_max_steps,\n",
    "    prior_dirichlet_concentration=PRIOR_DIRICHLET_CONCETRATION,\n",
    "    prior_dirichlet_concentration_map=PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "    n_steps_per_plot=N_STEPS_INTERVAL,\n",
    "    max_packets_transmitted=4,\n",
    "    true_mpr_matrix=MPR_MATRIX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9b864",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimize policies on model\n",
    "\n",
    "### Model as posterior sample from bayesian estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f4fd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_metadata_policy_opt_on_posterior_model(lr_actor=POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"]):\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"posterior_sample\",\n",
    "                \"n_steps_between_model_update\": N_STEPS_BETWEEN_POSTERIOR_SAMPLE,\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": N_EPISODES_TRAINING_POLICY,\n",
    "            \"train_policy_max_steps\": N_STEPS_TRAINING_POLICY,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": {\n",
    "                **POLICY_OPTIMIZER_METADATA,\n",
    "                \"learning_rate_actor\": lr_actor\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_policy_opt_on_posterior_model(n_model, n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_policy_opt_on_posterior_model_{EXPERIMENT_VERSION}_n_model_{n_model}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b9a12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    policy_opt_experiment_name = get_experiment_name_policy_opt_on_posterior_model(n_model, n_steps)\n",
    "    model_experiment_name = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    metadata = get_metadata_policy_opt_on_posterior_model()\n",
    "    if does_experiment_exist(policy_opt_experiment_name):\n",
    "        print(f\"Experiment '{policy_opt_experiment_name}' already done...\")\n",
    "    else:\n",
    "        try:\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=LOG_POLICY_TRAINING,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "                suffix_log_experiment_name=False\n",
    "            )\n",
    "        except:\n",
    "            print(\"ERROR ! Trying with half the learning rate for the actor...\")\n",
    "            if does_experiment_exist(policy_opt_experiment_name):\n",
    "                os.rename(\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, policy_opt_experiment_name),\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{policy_opt_experiment_name}_ERROR\"),\n",
    "                )\n",
    "            new_actor_lr = POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"] / 2\n",
    "            metadata = get_metadata_policy_opt_on_posterior_model(new_actor_lr)\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=LOG_POLICY_TRAINING,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "                suffix_log_experiment_name=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952b7bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model as maximum likelihood estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1609b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_metadata_policy_opt_on_max_likelihood_model(lr_actor=POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"]):\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION,\n",
    "                \"model_sampling_method\": \"maximum_likelihood\",\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": N_EPISODES_TRAINING_POLICY,\n",
    "            \"train_policy_max_steps\": N_STEPS_TRAINING_POLICY,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": {\n",
    "                **POLICY_OPTIMIZER_METADATA,\n",
    "                \"learning_rate_actor\": lr_actor\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_policy_opt_on_max_likelihood_model(n_model, n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_policy_opt_on_max_likelihood_model_{EXPERIMENT_VERSION}_n_model_{n_model}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596087a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    policy_opt_experiment_name = get_experiment_name_policy_opt_on_max_likelihood_model(n_model, n_steps)\n",
    "    model_experiment_name = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    metadata = get_metadata_policy_opt_on_max_likelihood_model()\n",
    "    if does_experiment_exist(policy_opt_experiment_name):\n",
    "        print(f\"Experiment '{policy_opt_experiment_name}' already done...\")\n",
    "    else:\n",
    "        try:\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=LOG_POLICY_TRAINING,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "                suffix_log_experiment_name=False\n",
    "            )\n",
    "        except:\n",
    "            print(\"ERROR ! Trying with half the learning rate for the actor...\")\n",
    "            if does_experiment_exist(policy_opt_experiment_name):\n",
    "                os.rename(\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, policy_opt_experiment_name),\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{policy_opt_experiment_name}_ERROR\"),\n",
    "                )\n",
    "            new_actor_lr = POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"] / 2\n",
    "            metadata = get_metadata_policy_opt_on_max_likelihood_model(new_actor_lr)\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=LOG_POLICY_TRAINING,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "                suffix_log_experiment_name=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288e537",
   "metadata": {},
   "source": [
    "### Model as maximum a posteriori estimator (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b49ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_policy_opt_on_map_model(lr_actor=POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"]):\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinSeparateModel\",\n",
    "            \"digital_twin_kwargs\": {\n",
    "                \"n_packets_rollouts\": N_PACKETS,\n",
    "                \"prior_dirichlet_concentration\": PRIOR_DIRICHLET_CONCETRATION_MAP,\n",
    "                \"model_sampling_method\": \"maximum_a_posteriori\",\n",
    "                \"exploration_policy_type\": \"random\"\n",
    "            },\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": N_EPISODES_TRAINING_POLICY,\n",
    "            \"train_policy_max_steps\": N_STEPS_TRAINING_POLICY,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": {\n",
    "                **POLICY_OPTIMIZER_METADATA,\n",
    "                \"learning_rate_actor\": lr_actor\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_policy_opt_on_map_model(n_model, n_steps):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_policy_opt_on_map_model_{EXPERIMENT_VERSION}_n_model_{n_model}_n_steps_{n_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81791691",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "    policy_opt_experiment_name = get_experiment_name_policy_opt_on_map_model(n_model, n_steps)\n",
    "    model_experiment_name = get_model_learning_experiment_name(n_model, n_steps)\n",
    "    metadata = get_metadata_policy_opt_on_map_model()\n",
    "    if does_experiment_exist(policy_opt_experiment_name):\n",
    "        print(f\"Experiment '{policy_opt_experiment_name}' already done...\")\n",
    "    else:\n",
    "        try:\n",
    "            run_simulation(\n",
    "                metadata,\n",
    "                log=True,\n",
    "                log_train=LOG_POLICY_TRAINING,\n",
    "                log_trained_model_or_policy=True,\n",
    "                log_experiment_name=policy_opt_experiment_name,\n",
    "                load_model_experiment_name=model_experiment_name,\n",
    "                load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "                suffix_log_experiment_name=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"ERROR ! Trying with half the learning rate for the actor...\")\n",
    "            if does_experiment_exist(policy_opt_experiment_name):\n",
    "                os.rename(\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, policy_opt_experiment_name),\n",
    "                    os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{policy_opt_experiment_name}_ERROR\"),\n",
    "                )\n",
    "            new_actor_lr = POLICY_OPTIMIZER_METADATA[\"learning_rate_actor\"] / 2\n",
    "            metadata = get_metadata_policy_opt_on_map_model(new_actor_lr)\n",
    "            retry = True\n",
    "            for n_attempt in range(5):  # 5 attempts with halved learning rate\n",
    "                if retry:\n",
    "                    try:\n",
    "                        run_simulation(\n",
    "                            metadata,\n",
    "                            log=True,\n",
    "                            log_train=LOG_POLICY_TRAINING,\n",
    "                            log_trained_model_or_policy=True,\n",
    "                            log_experiment_name=policy_opt_experiment_name,\n",
    "                            load_model_experiment_name=model_experiment_name,\n",
    "                            load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "                            suffix_log_experiment_name=False\n",
    "                        )\n",
    "                        retry = False\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(f\"ERROR ! Retrying with halved learning rate for the actor (attempt {n_attempt})...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d48a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \n",
    "a = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545a027",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model free baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391392b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_TRAININGS_MODEL_FREE_BASELINE = 50\n",
    "\n",
    "def get_metadata_model_free_baseline():\n",
    "    return Metadata.from_dict({\n",
    "        \"env_metadata\": ENV_METADATA,\n",
    "        # Policy\n",
    "        \"train_metadata\": {\n",
    "            \"digital_twin_class\": \"DigitalTwinPolicyPassthrough\",\n",
    "            \"digital_twin_kwargs\": {},\n",
    "            \"train_model_n_episodes\": 0,\n",
    "            \"train_model_max_steps\": 0,\n",
    "            \"train_policy_n_episodes\": N_EPISODES_TRAINING_POLICY,\n",
    "            \"train_policy_max_steps\": N_STEPS_TRAINING_POLICY,\n",
    "            \"policy_optimizer_class\": \"PolicyOptimizerCOMA\",\n",
    "            \"policy_optimizer_kwargs\": POLICY_OPTIMIZER_METADATA,\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_experiment_name_model_free_baseline(n_training):\n",
    "    return f\"{EXPERIMENT_NAME_PREFIX}_policy_opt_model_free_baseline_{EXPERIMENT_VERSION}_n_training_{n_training}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137c5d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_training in range(N_TRAININGS_MODEL_FREE_BASELINE):\n",
    "    policy_opt_experiment_name = get_experiment_name_model_free_baseline(n_training)\n",
    "    metadata = get_metadata_model_free_baseline()\n",
    "    if does_experiment_exist(policy_opt_experiment_name):\n",
    "        print(f\"Experiment '{policy_opt_experiment_name}' already done...\")\n",
    "    else:\n",
    "        run_simulation(\n",
    "            metadata,\n",
    "            log=True,\n",
    "            log_train=True,\n",
    "            log_experiment_name=policy_opt_experiment_name,\n",
    "            load_forced_test_experiment_name=EXPERIMENT_NAME_TEST_EPISODES_MODEL_BASED,\n",
    "            suffix_log_experiment_name=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b59c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plot performance Metrics\n",
    "\n",
    "### Load experiments functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47dec94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SELECT_EPISODES = [\"ep_0\"]\n",
    "\n",
    "LIGHT_SELECTION = {\n",
    "    \"rewards\": True,\n",
    "    \"actions\": True,\n",
    "    \"info\": {\n",
    "        \"buffer_overflow\": True,\n",
    "        \"channel_collision\": True,\n",
    "    },\n",
    "    \"state\": True,\n",
    "    \"digital_twin_info\": {},\n",
    "    \"train_info\": {}\n",
    "}\n",
    "\n",
    "def load_model_free_test_experiments():\n",
    "    model_free_test_episodes = [\n",
    "        Episode.load_experiment(\n",
    "            get_experiment_name_model_free_baseline(n_training),\n",
    "            experiment_phase=ExperimentPhase.TEST_POLICY,\n",
    "            light_selection=LIGHT_SELECTION,\n",
    "            select_episodes=SELECT_EPISODES\n",
    "        )\n",
    "        for n_training in range(N_TRAININGS_MODEL_FREE_BASELINE)\n",
    "    ] * len(N_STEPS_RANGE)\n",
    "    print(\"Model free test episodes loaded !\")\n",
    "    return model_free_test_episodes\n",
    "\n",
    "def load_posterior_test_experiments_generator():\n",
    "    for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "        yield Episode.load_experiment(\n",
    "            get_experiment_name_policy_opt_on_posterior_model(n_model, n_steps),\n",
    "            experiment_phase=ExperimentPhase.TEST_POLICY,\n",
    "            light_selection=LIGHT_SELECTION,\n",
    "            select_episodes=SELECT_EPISODES\n",
    "        )\n",
    "        if n_steps == N_STEPS_MAX:\n",
    "            print(f\"Posterior model {n_model} loaded...\")\n",
    "\n",
    "def load_frequentist_test_experiments_generator(map_estimator=True):\n",
    "    get_experiment_name_func = (\n",
    "        get_experiment_name_policy_opt_on_map_model if map_estimator\n",
    "        else get_experiment_name_policy_opt_on_max_likelihood_model\n",
    "    )\n",
    "    for n_model, n_steps in RANGE_MODEL_STEPS:\n",
    "        yield Episode.load_experiment(\n",
    "            get_experiment_name_func(n_model, n_steps),\n",
    "            experiment_phase=ExperimentPhase.TEST_POLICY,\n",
    "            light_selection=LIGHT_SELECTION,\n",
    "            select_episodes=SELECT_EPISODES\n",
    "        )\n",
    "        if n_steps == N_STEPS_MAX:\n",
    "            print(f\"{'MAP' if map_estimator else 'Max likelihood'} model {n_model} loaded...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a143f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe5bd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USE_MAP_ESTIMATOR = True\n",
    "\n",
    "INDIVIDUAL_PLOTS = [\n",
    "    (\n",
    "        plt.subplots(1, 1),\n",
    "        {\n",
    "            \"plot_return\": i == 0,\n",
    "            \"plot_throughput\": i == 1,\n",
    "            \"plot_buffer\": i == 2,\n",
    "            \"plot_collision\": i == 3,\n",
    "            \"plot_overflow\": i == 4,\n",
    "        }\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "PLOT_KWARGS ={\n",
    "    \"plot_buffer_max\": False,\n",
    "    \"range_return\": [-1700, 600],\n",
    "    \"range_throughput\": [-0.1, 1.15],\n",
    "    \"range_buffer\": [55, 105],\n",
    "    \"range_overflow\": [0.1, 0.45],\n",
    "    \"range_collision\": [0, 0.5],\n",
    "}\n",
    "\n",
    "def load_experiments_and_compute_plots():\n",
    "    # Plot arguments\n",
    "    # --------------\n",
    "    plot_args = [\n",
    "        N_STEPS_RETURN_PLOT,\n",
    "        RETURN_DISCOUNT,\n",
    "        N_PACKETS_MAX\n",
    "    ]\n",
    "    n_steps_range_model_free = []\n",
    "    for n_steps in N_STEPS_RANGE:\n",
    "        n_steps_range_model_free += ([n_steps] * N_TRAININGS_MODEL_FREE_BASELINE)\n",
    "    n_steps_range_all_models = [n_steps for n_model, n_steps in RANGE_MODEL_STEPS]\n",
    "\n",
    "\n",
    "    # MODEL FREE BASELINE\n",
    "    # -------------------\n",
    "    print(\"PLOTTING MODEL FREE BASELINE\")\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Loading test experiments\")\n",
    "    experiments_policy_opt_model_free = load_model_free_test_experiments()\n",
    "    print(\"Getting plots\")\n",
    "    for subplot, plot_selection_kwargs in INDIVIDUAL_PLOTS:\n",
    "        plot_validation_metrics_per_n_steps_model_learning(\n",
    "            subplot,\n",
    "            n_steps_range_model_free,\n",
    "            experiments_policy_opt_model_free,\n",
    "            *plot_args,\n",
    "            lineplot_kwargs={\"color\": \"blue\"},\n",
    "            annotate_x_pos=12,\n",
    "            annotate_label=\"Oracle-aided\",\n",
    "            **PLOT_KWARGS,\n",
    "            **plot_selection_kwargs\n",
    "        )\n",
    "    print(\"Freeing memory\")\n",
    "    del experiments_policy_opt_model_free\n",
    "\n",
    "    # BAYESIAN MODEL BASED\n",
    "    # --------------------\n",
    "    print(\"PLOTTING BAYESIAN MODEL BASED\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Loading test experiments\")\n",
    "    experiments_policy_opt_posterior_model = [\n",
    "        experiment for experiment in load_posterior_test_experiments_generator()\n",
    "    ]\n",
    "    print(\"Getting plots\")\n",
    "    for subplot, plot_selection_kwargs in INDIVIDUAL_PLOTS:\n",
    "        plot_validation_metrics_per_n_steps_model_learning(\n",
    "            subplot,\n",
    "            n_steps_range_all_models,\n",
    "            experiments_policy_opt_posterior_model,\n",
    "            *plot_args,\n",
    "            lineplot_kwargs={\"color\": \"orange\"},\n",
    "            annotate_x_pos=9,\n",
    "            annotate_label=\"Bayesian\",\n",
    "            **PLOT_KWARGS,\n",
    "            **plot_selection_kwargs\n",
    "        )\n",
    "    print(\"Freeing memory\")\n",
    "    del experiments_policy_opt_posterior_model\n",
    "\n",
    "    # FRQUENTIST MODEL BASED\n",
    "    # --------------------\n",
    "    print(\"PLOTTING BAYESIAN MODEL BASED\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Loading test experiments\")\n",
    "    experiments_policy_opt_max_likelihood_model = [\n",
    "        experiment for experiment in load_frequentist_test_experiments_generator(map_estimator=USE_MAP_ESTIMATOR)\n",
    "    ]\n",
    "    print(\"Getting plots\")\n",
    "    for subplot, plot_selection_kwargs in INDIVIDUAL_PLOTS:\n",
    "        plot_validation_metrics_per_n_steps_model_learning(\n",
    "            subplot,\n",
    "            n_steps_range_all_models,\n",
    "            experiments_policy_opt_max_likelihood_model,\n",
    "            *plot_args,\n",
    "            lineplot_kwargs={\"color\": \"green\"},\n",
    "            annotate_x_pos=6,\n",
    "            annotate_label=\"Frequentist\",\n",
    "            **PLOT_KWARGS,\n",
    "            **plot_selection_kwargs\n",
    "        )\n",
    "    print(\"Freeing memory\")\n",
    "    del experiments_policy_opt_max_likelihood_model\n",
    "\n",
    "    # Plot params\n",
    "    print(\"Setting up plot parameters\")\n",
    "    figsize = (10, 4)\n",
    "    for subplot, _ in INDIVIDUAL_PLOTS:\n",
    "        fig, ax = subplot\n",
    "        fig.set_size_inches(*figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c59759",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_experiments_and_compute_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22019904",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102cef1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return\n",
    "INDIVIDUAL_PLOTS[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c8466",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Throughput\n",
    "INDIVIDUAL_PLOTS[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cd276",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Buffer Occupancy\n",
    "INDIVIDUAL_PLOTS[2][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c273c56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Overflow\n",
    "INDIVIDUAL_PLOTS[4][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346bd50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Collision\n",
    "INDIVIDUAL_PLOTS[3][0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt-mpr-channel",
   "language": "python",
   "name": "dt-mpr-channel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
